\documentclass[10pt, a4paper, landscape]{extarticle}

% ----- packages -----
\usepackage{amsmath, amsfonts, amssymb} % better math
\usepackage{enumitem} % better lists
\usepackage{geometry} % margins
\usepackage{graphicx} % scaling
\usepackage{hyperref} % hyperlinks
\usepackage{multicol} % multiple columns
\usepackage{parskip} % paragraph spacing
\usepackage{scrlayer-scrpage} % page foot
\usepackage{tikz} % plots
\usepackage{titlesec} % titles

% ----- random seed -----
\pgfmathsetseed{12}

% ----- custom commands -----
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\se}{\mathrm{ee}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\SSR}{\mathrm{SRC}}
\newcommand{\SSE}{\mathrm{SEC}}
\newcommand{\SST}{\mathrm{STC}}
\newcommand{\tr}{\mathsf{T}}

% ----- page customization -----
\geometry{margin=1cm} % margins config
\pagenumbering{gobble} % remove page numeration
\setlength{\parskip}{0cm} % paragraph spacing
% title spacing
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1ex}{0ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}

% ----- document -----
\begin{document}

\cfoot{\href{https://github.com/marcelomijas/econometrics-cheatsheet}{\normalfont \footnotesize 3.2-es - github.com/marcelomijas/econometrics-cheatsheet - Licencia CC-BY-4.0}}
\setlength{\footskip}{12pt}

\begin{multicols}{3}

\begin{center}
	\textbf{\LARGE \href{https://github.com/marcelomijas/econometrics-cheatsheet}{Cheat Sheet Econometría}}
	
	{\footnotesize Por Marcelo Moreno - Universidad Rey Juan Carlos}
	
	{\footnotesize The Econometrics Cheat Sheet Project}
\end{center}

\section*{Conceptos básicos}

\subsection*{Definiciones}

\textbf{Econometría} - es una disciplina de las ciencias sociales que tiene como objetivo cuantificar las relaciones entre agentes económicos, contrastar teorías económicas y evaluar e implementar políticas públicas y privadas.

\textbf{Modelo econométrico} - es una representación simplificada de la realidad para explicar fenómenos económicos.

\textbf{\textsl{Ceteris paribus}} - si todos los demás factores relevantes permanecen constantes.

\subsection*{Tipos de datos}

\textbf{Sección cruzada} - datos recogidos en un momento dado en el tiempo, una \textsl{foto} estática. El orden no importa.

\textbf{Series temporales} - observación de una/muchas variable/s durante un periodo de tiempo. El orden sí importa.

\textbf{Datos de panel} - consiste una una serie temporal por cada observación de una sección cruzada.

\textbf{Secciones transversales agrupadas} - combina secciones cruzadas de diferentes periodos temporales.

\subsection*{Fases de un modelo econométrico}

\begin{enumerate}[leftmargin=*]
	\setlength{\multicolsep}{0pt}
	\begin{multicols}{2}
		\item Especificación.
		\item Estimación.
	\columnbreak
		\item Validación.
		\item Utilización.
	\end{multicols}
\end{enumerate}

\subsection*{Análisis de regresión}

Estudiar y predecir el valor medio de una variable (dependiente, $y$) respecto a unos valores fijos de otras variables (variables independientes, $x$'s). En econometría es común usar Mínimos Cuadrados Ordinarios (MCO) para análisis de regresión.

\subsection*{Análisis de correlación}

El análisis de correlación no distingue entre variables dependientes e independientes.

\begin{itemize}[leftmargin=*]
	\item La correlación simple mide el grado de asociación lineal entre dos variables.
	\begin{center}
		$r = \frac{\Cov(x, y)}{\sigma_x \cdot \sigma_y} = \frac{\sum_{i=1}^n ((x_i - \overline{x}) \cdot (y_i - \overline{y}))}{\sqrt{\sum_{i=1}^n (x_i - \overline{x})^2 \cdot \sum_{i=1}^n (y_i - \overline{y})^2}}$
	\end{center}
	\item La correlación parcial mide el grado de de asociación lineal entre dos variables controlando una tercera.
\end{itemize}

\columnbreak

\section*{Supuestos y propiedades}

\subsection*{Supuestos del modelo econométrico}

Bajo estos supuestos, el estimador de MCO presentará buenas propiedades. Supuestos \textbf{Gauss-Markov}:

\begin{enumerate}[leftmargin=*]
	\item \textbf{Linealidad en parámetros} (y dependencia débil en series temporales). $y$ debe ser una función lineal de $\beta$'s.
	\item \textbf{Muestreo aleatorio}. La muestra de la población se ha tomado de forma aleatoria. (Sólo sección cruzada)
	\item \textbf{No colinealidad perfecta}.
	\begin{itemize}[leftmargin=*]
		\item No hay variables independientes que sean constantes: $\Var(x_j) \neq 0, \; \forall j = 1, \ldots, k$.
		\item No hay una relación lineal exacta entre variables independientes.
	\end{itemize}
	\item \textbf{Media condicional cero y correlación cero}.
	\begin{enumerate}[leftmargin=*, label=\alph*.]
		\item No hay errores sistemáticos: $\E(u \mid x_1, \ldots, x_k) = \E(u) = 0 \rightarrow$ \textbf{exogeneidad fuerte} (a implica b).
		\item No hay variables relevantes fuera del modelo: $\Cov(x_j, u) = 0, \; \forall j = 1, \ldots, k \rightarrow$ \textbf{exogeneidad débil}.
	\end{enumerate}
	\item \textbf{Homocedasticidad}. La variabilidad de los residuos es igual para todos los niveles de $x$: \\ $\Var(u \mid x_1, \ldots, x_k) = \sigma^2_u$
	\item \textbf{No autocorrelación}. Los residuos no contienen información sobre otros residuos: \\ $\Corr(u_t, u_s \mid x_1, \ldots, x_k) = 0, \; \forall t \neq s$.
	\item \textbf{Normalidad}. Los residuos son independientes e idénticamente distribuidos: $u \sim \mathcal{N}(0, \sigma^2_u)$
	\item \textbf{Tamaño de datos}. El número de observaciones disponibles debe ser mayor a $(k + 1)$ parámetros a estimar. (Ya satisfecho bajo situaciones asintóticas)
\end{enumerate}

\subsection*{Propiedades asintóticas de MCO}

Bajo los supuestos del modelo econométrico y el Teorema Central del Límite (TCL):

\begin{itemize}[leftmargin=*]
	\item De 1 a 4a: MCO es \textbf{insesgado}. $\E(\hat{\beta}_j) = \beta_j$
	\item De 1 a 4: MCO es \textbf{consistente}. $\mathrm{plim}(\hat{\beta}_j) = \beta_j$ (a 4b sin 4a, exogeneidad débil, insesgado y consistente).
	\item De 1 a 5: \textbf{normalidad asintótica} de MCO (entonces, 7 es necesariamente satisfecho): $u \underset{a}{\sim} \mathcal{N}(0, \sigma^2_u)$
	\item De 1 a 6: \textbf{estimador insesgado} de $\sigma^2_u$. $\E(\hat{\sigma}^2_u) = \sigma^2_u$
	\item De 1 a 6: MCO es \textcolor{blue}{\href{https://www.youtube.com/watch?v=68ugkg9RePc}{MELI}} (Mejor Estimador Lineal Insesgado) ó \textbf{eficiente}. 
	\item De 1 a 7: contrastes de hipótesis e intervalos de confianza son fiables.
\end{itemize}

\section*{Mínimos Cuadrados Ordinarios}

\textbf{Objetivo} - minimizar Suma de Resid. Cuadrados (SRC):

\begin{center}
	$\min \sum_{i=1}^n \hat{u}_i^2$, donde $\hat{u}_i = y_i - \hat{y}_i$
\end{center}

\subsection*{Modelo de regresión simple}

\setlength{\multicolsep}{2pt}
\setlength{\columnsep}{-40pt}
\begin{multicols}{2}

\begin{tikzpicture}[scale=0.15]
	% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % background grid
	\draw [thick, <->] (-10, 10) node [anchor=south] {$y$} -- (-10, -10) -- (10, -10) node [anchor=north] {$x$}; %axis
	\draw [red, thick] plot [domain=-10:10] (\x, {1 + 0.5*\x}); % regression line
	\draw plot [only marks, mark=*, mark size=6, domain=-8:8, samples=20] (\x, {rnd*5 - 1.5 + 0.5*\x}); % data points
	\draw (-9.3, -9.6) -- (-9.5, -9.6) -- (-9.5, -4.3) -- (-9.3, -4.3) node [anchor=north west] {$\beta_0$}; % beta0
	\draw (-2, 0) -- (1.5, 0) arc (0:25:3.5); % beta1 arc
	\draw (3, -0.5) node {$\beta_1$}; % beta1
\end{tikzpicture}

\columnbreak

Ecuación:

\begin{center}
	$y_i = \beta_0 + \beta_1 x_i + u_i$
\end{center}

Estimación:

\begin{center}
	$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$
\end{center}

donde:

\begin{center}
	$\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$
	
	$\hat{\beta}_1 = \frac{\Cov(y, x)}{\Var(x)}$
\end{center}

\end{multicols}

\subsection*{Modelo de regresión múltiple}

\setlength{\multicolsep}{2pt}
\setlength{\columnsep}{-40pt}
\begin{multicols}{2}

\begin{tikzpicture}[scale=0.15]
	\draw [thick, ->] (-10, -10) -- (-3, -4) node [anchor=north west] {$x_2$}; % x2 axis
	\draw [thick, <->] (-10, 10) node [anchor=south] {$y$} -- (-10, -10) -- (10, -10) node [anchor=north] {$x_1$}; % y and x1 axis
	% regression grid
	\draw [red, thick] (-10, -4) -- (-5, 3);
	\draw [red, thick] (-8.5, -3.5) -- (-3.5, 3.5);
	\draw [red, thick] (-7, -3) -- (-2, 4);
	\draw [red, thick] (-5.5, -2.5) -- (-0.5, 4.5);
	\draw [red, thick] (-4, -2) -- (1, 5);
	\draw [red, thick] (-2.5, -1.5) -- (2.5, 5.5);
	\draw [red, thick] (-1, -1) -- (4, 6);
	\draw [red, thick] (0.5, -0.5) -- (5.5, 6.5);
	\draw [red, thick] (2, 0) -- (7, 7);
	\draw [red, thick] (3.5, 0.5) -- (8.5, 7.5);
	\draw [red, thick] (5, 1) -- (10, 8);
	\draw [red, thick] (-10, -4) -- (5, 1);
	\draw [red, thick] (-8.75, -2.25) -- (6.25, 2.75);
	\draw [red, thick] (-7.5, -0.5) -- (7.5, 4.5);
	\draw [red, thick] (-6.25, 1.25) -- (8.75, 6.25);
	\draw [red, thick] (-5, 3) -- (10, 8);
	\draw plot [only marks, mark=*, mark size=6, domain=-8:8, samples=20] (\x, {rnd*6.5 - 1.5 + 0.5*\x}); % data points
	\draw (-9.3, -9.6) -- (-9.5, -9.6) -- (-9.5, -4.3) -- (-9.3, -4.3) node [anchor=north west] {$\beta_0$}; % beta0
\end{tikzpicture}

\columnbreak

Ecuación:

\begin{center}
	$y_i = \beta_0 + \beta_1 x_{1i} + \cdots + \beta_k x_{ki} + u_i$
\end{center}

Estimación:

\begin{center}
	$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1i} + \cdots + \hat{\beta}_k x_{ki}$
\end{center}

donde:

\begin{center}
	$\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}_1 - \cdots - \hat{\beta}_k \overline{x}_k$
	
	$\hat{\beta}_j = \frac{\Cov(y, \text{resid } x_j)}{\Var(\text{resid } x_j)}$
\end{center}

Matriz: $\hat{\beta} = (X^\tr X)^{-1} (X^\tr y)$

\end{multicols}

\subsection*{Interpretación de coeficientes}

\begin{center}
	\scalebox{0.85}{
		\begin{tabular}{ c c c c }
			Modelo      & Depend.   & Independ. & Interpretación $\beta_1$                       \\ \hline
			Nivel-nivel & $y$       & $x$       & $\Delta y = \beta_1 \Delta x$                  \\
			Nivel-log   & $y$       & $\log(x)$ & $\Delta y \approx (\beta_1/100) (\% \Delta x$) \\
			Log-nivel   & $\log(y)$ & $x$       & $\% \Delta y \approx (100 \beta_1) \Delta x$   \\
			Log-log     & $\log(y)$ & $\log(x)$ & $\% \Delta y \approx \beta_1 (\% \Delta x$)    \\
			Cuadrático  & $y$       & $x + x^2$ & $\Delta y = (\beta_1 + 2 \beta_2 x) \Delta x$
		\end{tabular}
	}
\end{center}

\subsection*{Medidas de error}

Suma de Resid. Cuad.: \hfill $\SSR = \sum_{i=1}^n \hat{u}_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2$

Suma Explicada de Cuadrados: \hfill $\SSE = \sum_{i=1}^n (\hat{y}_i - \overline{y})^2$

Suma Tot. de Cuad.: \hfill $\SST = \SSE + \SSR = \sum_{i=1}^n (y_i - \overline{y})^2$

Error Estándar de la Regresión: \hfill $\hat{\sigma}_u = \sqrt{\frac{\SSR}{n - k - 1}}$

Error Estándar de los $\hat{\beta}$'s: \hfill $\se(\hat{\beta}) = \sqrt{\hat{\sigma}^2_u \cdot (X^\tr X)^{-1}}$

Error Cuadrático Medio: \hfill $\mathrm{ECM} = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}$

Error Medio Absoluto: \hfill $\mathrm{EMA} = \frac{\sum_{i=1}^n \lvert y_i - \hat{y}_i \rvert}{n}$

Porcentaje Medio de Error: \hfill $\mathrm{PME} = \frac{\sum_{i=1}^n \lvert \hat{u}_i / y_i \rvert}{n} \cdot 100$

\columnbreak

\section*{R-cuadrado}

Es una medida de la \textbf{bondad del ajuste}, cómo la regresión se ajusta a los datos:

\begin{center}
	$R^2 = \frac{\SSE}{\SST} = 1 - \frac{\SSR}{\SST}$
\end{center}

\begin{itemize}[leftmargin=*]
	\item Mide el \textbf{porcentaje de variación} en $y$ que es linealmente \textbf{explicado} por variaciones de las $x$'s.
	\item Toma valores \textbf{entre 0} (no hay explicación lineal de las variaciones de $y$) \textbf{y 1} (explicación total de las variaciones de $y$).
\end{itemize}

Cuando el número de regresores incrementa, el valor del R-cuadrado también lo hace, independientemente de si las nuevas variables son relevantes o no. Para resolver este problema, existe un \textbf{R-cuadrado ajustado} por grados de libertad (o R-cuadrado corregido):

\begin{center}
	$\overline{R}^2 = 1 - \frac{n - 1}{n - k - 1} \cdot \frac{\SSR}{\SST} = 1 - \frac{n - 1}{n - k - 1} \cdot (1 - R^2)$
\end{center}

Para muestras grandes: $\overline{R}^2 \approx R^2$

\section*{Contrastes de hipótesis}

\subsection*{Definiciones}

Un contraste de hipótesis es una regla diseñada para, a partir de una muestra, explicar si existe \textbf{evidencia para rechazar (o no) una hipótesis} sobre uno o más parámetros poblacionales.

Elementos de un contraste de hipótesis:

\begin{itemize}[leftmargin=*]
	\item \textbf{Hipótesis nula} ($H_0$) - es la hipótesis a ser probada.
	\item \textbf{Hipótesis alternativa} ($H_1$) - el la hipótesis que no puede rechazarse si la hipótesis nula es rechazada.
	\item \textbf{Estadístico de contraste} - es una variable aleatoria cuya distribución de probabilidad es conocida bajo la hipótesis nula.
	\item \textbf{Nivel de significación} ($\alpha$) - es la probabilidad de rechazar la hipótesis nula siendo cierta (Error Tipo I). Es elegido por quien conduce el contraste. Usualmente es 0.10, 0.05 ó 0.01.
	\item \textbf{Valor crítico} - es el valor contra el cual se compara el estadístico de contraste para determinar si se rechaza o no la hipótesis nula. Es el valor frontera entre la región de aceptación y la de rechazo de la hipótesis nula.
	\item \textbf{p-valor} - es el nivel de significación máximo por el cual la hipótesis nula no puede ser rechazada ($H_0$).
\end{itemize}

\textbf{Regla general}: si el p-valor es \textbf{menor} que $\alpha$, existe evidencia para \textbf{rechazar} la \textbf{hipótesis nula} a ese determinado $\alpha$ (existe evidencia para aceptar la hipótesis alternativa).

\subsection*{Contrastes individuales}

Prueba si un parámetro es significativamente diferente de un cierto valor, $\vartheta$.

\begin{itemize}[leftmargin=*]
	\item $H_0: \beta_j = \vartheta$
	\item $H_1: \beta_j \neq \vartheta$
\end{itemize}

\begin{center}
	Bajo $H_0$: \quad $t = \frac{\hat{\beta}_j - \vartheta}{\se(\hat{\beta}_j)} \sim t_{n - k - 1, \alpha/2}$
\end{center}

Si $\lvert t \rvert > \lvert t_{n - k - 1, \alpha/2} \rvert$, existe evidencia para rechazar $H_0$.

\textbf{Contraste de significación individual} - prueba si un parámetro es \textbf{significativamente distinto de cero}.

\begin{itemize}[leftmargin=*]
	\item $H_0: \beta_j = 0$
	\item $H_1: \beta_j \neq 0$
\end{itemize}

\begin{center}
	Bajo $H_0$: \quad $t = \frac{\hat{\beta}_j}{\se(\hat{\beta}_j)} \sim t_{n - k - 1, \alpha/2}$
\end{center}

Si $\lvert t \rvert > \lvert t_{n - k - 1, \alpha/2} \rvert$, existe evidencia para rechazar $H_0$.

\subsection*{Contraste F}

Prueba simultáneamente múltiples hipótesis (lineales) sobre los parámetros. Hace uso de un modelo no restringido y uno restringido:

\begin{itemize}[leftmargin=*]
	\item \textbf{Modelo no restringido} - es el modelo donde se quiere probar la hipótesis.
	\item \textbf{Modelo restringido} - es el modelo donde se ha impuesto la hipótesis que se quiere probar.
\end{itemize}

Entonces, viendo los errores, hay:

\begin{itemize}[leftmargin=*]
	\item \textbf{$\SSR_\mathrm{UR}$} - es la $\SSR$ del modelo no restringido.
	\item \textbf{$\SSR_\mathrm{R}$} - es la $\SSR$ del modelo restringido.
\end{itemize}

\begin{center}
	Bajo $H_0$: \quad $F = \frac{\SSR_\mathrm{R} - \SSR_\mathrm{UR}}{\SSR_\mathrm{UR}} \cdot \frac{n - k - 1}{q} \sim F_{q, n - k - 1}$
\end{center}

donde $k$ es el número de parámetros del modelo no restringido y $q$ es el número de hipótesis lineales a probar.

Si $F_{q, n - k - 1} < F$, existe evidencia para rechazar $H_0$.

\textbf{Contraste de significación global} - prueba si todos los parametros asociados a $x$'s son \textbf{simultáneamente iguales a cero}.

\begin{itemize}[leftmargin=*]
	\item $H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0$
	\item $H_1: \beta_1 \neq 0$ y/o $\beta_2 \neq 0 \ldots$ y/o $\beta_k \neq 0$
\end{itemize}

En este caso, podemos simplificar la fórmula para el estadístico $F$.

\begin{center}
	Bajo $H_0$: \quad $F = \frac{R^2}{1 - R^2} \cdot \frac{n - k - 1}{k} \sim F_{k, n - k - 1}$
\end{center}

Si $F_{k, n - k - 1} < F$, existe evidencia para rechazar $H_0$.

\section*{Intervalos de confianza}

Los intervalos de confianza al nivel de confianza ($1 - \alpha$), se pueden calcular:

\begin{center}
	$\hat{\beta}_j \mp t_{n - k - 1, \alpha/2} \cdot \se(\hat{\beta}_j)$
\end{center}

\section*{Variables ficticias}

Las variables ficticias (o binarias) son usadas para recoger información cualitativa: sexo, estado civil, país, etc.

\begin{itemize}[leftmargin=*]
	\item Toman \textbf{valor 1} en una categoría dada y \textbf{0 en el resto}.
	\item Se usan para analizar y modelizar \textbf{cambios estructurales} en los parámetros del modelo.
\end{itemize}

Si una variable cualitativa tiene $m$ categorías, sólo hay que incluir ($m - 1$) variables ficticias en el modelo.

\subsection*{Cambio estructural}

El cambio estructural se refiere a los cambios en los valores de los parámetros del modelo producidos por el efecto de diferentes sub-poblaciones. El cambio estructural se puede incluir en el modelo a través de variables ficticias.

La ubicación de las variables ficticias ($D$) es importante:

\begin{itemize}[leftmargin=*]
	\item \textbf{En la constante} (efecto aditivo) - representa la diferencia media entre los valores producidos por el cambio estructural.
	\begin{center}
		$y = \beta_0 + \delta_1 D + \beta_1 x_1 + u$
	\end{center}
	\item \textbf{En la pendiente} (efecto multiplicativo) - representa la diferencia en el efecto (pendiente) entre los valores producidos por el cambio estructural.
	\begin{center}
		$y = \beta_0 + \beta_1 x_1 + \delta_1 D \cdot x_1 + u$
	\end{center}
\end{itemize}

El \textbf{contraste de Chow para cambio estructural} - cuando se quiere analizar la existencia de cambio estructural en todos los parámetros del modelo, es una expresión particular del contraste F, conocido como el contraste de Chow, donde la hipótesis nula es: $H_0$: No hay cambio estructural (todos $\delta = 0$).

\section*{Cambios de escala}

Cambios en las \textbf{unidades de medida} de las variables:

\begin{itemize}[leftmargin=*]
	\item Sobre la variable \textbf{endógena}, $y^* = y \cdot \lambda$ - afecta a todos los parámetros del modelo, $\beta_j^* = \beta_j \cdot \lambda, \; \forall j = 1, \ldots, k$
	\item Sobre una variable \textbf{exógena}, $x_j^* = x_j \cdot \lambda$ - sólo afecta al parámetro ligado a dicha variable exógena, $\beta_j^* = \beta_j \cdot \lambda$
	\item Mismo cambio de escala sobre endógena y exógena - sólo afecta al término constante, $\beta_0^* = \beta_0 \cdot \lambda$
\end{itemize}

\section*{Cambios de origen}

Cambios en el \textbf{origen de medida} de las variables (endógenas o exógenas), $y^* = y + \lambda$ - sólo afectan al término constante del modelo, $\beta_0^* = \beta_0 + \lambda$

\columnbreak

\section*{Multicolinealidad}

\begin{itemize}[leftmargin=*]
	\item \textbf{Multicolinealidad perfecta} - hay variables independientes que son constantes y/o hay una relación lineal exacta entre variables independientes. Es el \textbf{incumplimiento del tercer (3) supuesto} del modelo.
	\item \textbf{Multicolinealidad aproximada} - hay variables independientes que son aproximadamente constantes y/o hay una relación lineal aproximada entre variables independientes. \textbf{No implica el incumplimiento de algún supuesto} del modelo, pero tiene un efecto en MCO.
\end{itemize}

\subsection*{Consecuencias}

\begin{itemize}[leftmargin=*]
	\item \textbf{Multicolinealidad perfecta} - el sistema de ecuaciones de MCO no puede resolverse (infinitas soluciones).
	\item \textbf{Multicolinealidad aproximada}
	\begin{itemize}[leftmargin=*]
		\item Pequeñas variaciones en la muestra producen grandes variaciones en las estimaciones de MCO.
		\item La varianza de los estimadores MCO de las $x$'s que son colineales incrementa, la inferencia de los parámetros es afectada (intervalo de confianza grande).
	\end{itemize}
\end{itemize}

\subsection*{Detección}

\begin{itemize}[leftmargin=*]
	\item \textbf{Análisis de correlación} - buscar altas correlaciones entre variables independientes, $> \lvert 0.7 \rvert$.
	\item \textbf{Factor de Inflación de la Varianza (FIV o VIF)} - indica el incremento en $\Var(\hat{\beta}_j)$ debido a la multicolinealidad.
	\begin{center}
			$\mathrm{VIF} (\hat{\beta}_j) = \frac{1}{1-R_j^2}$
	\end{center}
	donde $R^2_j$ denota el R-cuadrado de una regresión entre $x_j$ y todas las otras $x$'s. 
	\begin{itemize}[leftmargin=*]
		\item Valores entre 4 y 10 sugieren que es recomendable analizar en mayor profundidad si pueden existir problemas de multicolinealidad.
		\item Valores por encima de 10 indican que existen problemas de multicolinealidad.
	\end{itemize}
\end{itemize}

Una característica típica de la multicolinealidad es que los coeficientes de regresión del modelo no son individualmente significativos (por las altas varianzas), pero sí que son conjuntamente significativos.

\subsection*{Corrección}

\begin{itemize}[leftmargin=*]
			\item Eliminar una de las variables colineales.
			\item Realizar análisis factorial(u otra técnica de reducción de dimensiones) en las vairables colineales.
			\item Interpretar los coeficientes con multicolinealidad de manera conjunta.
\end{itemize}

\columnbreak

\section*{Heterocedasticidad}

Los residuos $u_i$ de la función de regresión poblacional no tienen una varianza constante $\sigma^2_u$:

\begin{center}
	$\Var(u \mid x_1, \ldots, x_k) = \Var(u) \neq \sigma^2_u$
\end{center}

Es el \textbf{incumplimiento del quinto (5) supuesto} del modelo.

\subsection*{Consecuencias}

\begin{itemize}[leftmargin=*]
	\item Estimadores MCO son insesgados.
	\item Estimadores MCO son consistentes.
	\item MCO ya \textbf{no es eficiente}, pero sigue siendo ELI (Estimador Lineal Insesgado).
	\item La \textbf{estimación de la varianza} de los estimadores es \textbf{sesgada}: la construcción de intervalos de confianza y contraste de hipótesis no son fiables.
\end{itemize}

\subsection*{Detección}

\begin{itemize}[leftmargin=*]
	\setlength{\multicolsep}{0pt}
	\setlength{\columnsep}{20pt}
	\begin{multicols}{3}
		\item \textbf{Gráficos} - buscar patrones de dispersión en gráficos $x$ vs. $u$ ó $x$ vs. $y$.
	\columnbreak
		\begin{tikzpicture}[scale=0.108]
			% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
			\draw [thick, ->] (-10, 0) -- (10, 0) node [anchor=north] {$x$}; % x axis
			\draw [thick, -] (-10, -10) -- (-10, 10) node [anchor=west] {$u$}; % u axis
			\draw plot [only marks, mark=*, mark size=6, domain=0:17, samples=50] ({\x - 9}, {-0.5*rand*\x - 1}); % data points
			\draw [thick, dashed, red, -latex] plot [domain=1:17] ({\x - 10}, {-0.5*\x - 1}); % lower red arrow
			\draw [thick, dashed, red, -latex] plot [domain=1:17] ({\x - 10}, {0.5*\x - 1}); % upper red arrow
		\end{tikzpicture}
	\columnbreak
		\begin{tikzpicture}[scale=0.108]
			% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
			\draw [thick, <->] (-10, 10) node [anchor=west] {$y$} -- (-10, -10) -- (10, -10) node[anchor=south] {$x$}; % axis
			\draw plot [only marks, mark=*, mark size=6, domain=0:13, samples=50] ({\x - 9}, {(-0.65*rand*\x) + 0.6*\x - 8}); % data points
			\draw [thick, dashed, red, -latex] plot [domain=0:12] ({\x - 9}, {-0.06*\x - 8.5}); % lower red arrow
			\draw [thick, dashed, red, -latex] plot [domain=0:12] ({\x -9}, {1.25*\x - 7.2}); % upper red arrow
		\end{tikzpicture}
	\end{multicols}
	\item \textbf{Test formales} - White, Bartlett, Breusch-Pagan, etc. Comúnmente, la hipótesis nula: $H_0$: Homocedasticidad.
\end{itemize}

\subsection*{Corrección}

\begin{itemize}[leftmargin=*]
	\item Usar MCO con un estimador de la matriz de varianzas-covarianzas robusto a la heterocedasticidad (HC), por ejemplo, la propuesta de White.
	\item Si la estructura de la varianza es conocida, usar Mínimos Cuadrados Ponderados (MCP) o Mínimos Cuadrados Generalizados (MCG):
	\begin{itemize}[leftmargin=*]
		\item Suponiendo que $\Var(u) = \sigma^2_u \cdot x_i$, dividir las variables del modelo entre la raíz cuadrada de $x_i$ y aplicar MCO.
		\item Suponiendo que $\Var(u) = \sigma^2_u \cdot x_i^2$, dividir las variables del modelo entre $x_i$ (la raíz cuadrada de $x_i^2$) y aplicar MCO.
	\end{itemize}
	\item Si la estructura de la varianza es desconocida, hacer uso de Mínimos Curadrados Ponderados Factibles (MCPF), que estima una posible varianza, divide las variables del modelo entre ella y entonces aplica MCO.
	\item Nueva especificación del modelo, por ejemplo, transformación logarítmica (reduce la varianza).
\end{itemize}

\columnbreak

\section*{Autocorrelación}

El residuo de cualquier observación, $u_t$, está correlacionado con el residuo de cualquier otra observación. Las observaciones no son independientes.

\begin{center}
	$\Corr(u_t, u_s \mid x_1, \ldots, x_k) = \Corr(u_t, u_s) \neq 0 \quad \forall t \neq s$
\end{center}

El contexto ``natural" de este fenómeno son las series temporales. Es el \textbf{incumplimiento del sexto (6) supuesto} del modelo.

\subsection*{Consecuencias}

\begin{itemize}[leftmargin=*]
	\item Estimadores MCO son insesgados.
	\item Estimadores MCO son consistentes.
	\item MCO ya \textbf{no es eficiente}, pero sigue siendo ELI (Estimador Lineal Insesgado).
	\item La \textbf{estimación de la varianza} de los estimadores es \textbf{sesgada}: la construcción de intervalos de confianza y contraste de hipótesis no son fiables.
\end{itemize}

\subsection*{Detección}

\begin{itemize}[leftmargin=*]
	\item \textbf{Gráficos} - buscar patrones de dispersión en gráficos $u_{t - 1}$ vs. $u_t$ o hacer uso del correlograma.
	\setlength{\multicolsep}{0pt}
	\setlength{\columnsep}{6pt}
	\begin{multicols}{3}
		\begin{center}
			\begin{tikzpicture}[scale=0.11]
				\node at (0, 13) {\textbf{Ac.}};
				% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
				\draw [thick, ->] (-10, 0) -- (10, 0) node [anchor=south] {$u_{t - 1}$}; % ut-1 axis
				\draw [thick, -] (-10, -10) -- (-10, 10) node [anchor=west] {$u_t$}; % ut axis
				\draw plot [only marks, mark=*, mark size=6, domain=-8:8, samples=50] (\x, {rnd*6 + (-2*(\x)^2 + 40)*0.1}); % data points
				\draw [thick, dashed, red, -latex] plot [domain=-8:8] (\x, {3 + (-2*(\x)^2 + 40)*0.1}); % red arrow
			\end{tikzpicture}
		\end{center}
	\columnbreak
		\begin{center}
			\begin{tikzpicture}[scale=0.11]
				\node at (0, 13) {\textbf{Ac.(+)}};
				% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
				\draw [thick, ->] (-10, 0) -- (10, 0) node [anchor=north] {$u_{t - 1}$}; % ut-1 axis
				\draw [thick, -] (-10, -10) -- (-10, 10) node [anchor=west] {$u_t$}; % ut axis
				\draw plot [only marks, mark=*, mark size=6, domain=-8:8, samples=25] (\x, {rnd*6 + 0.5*\x - 3}); % data points
				\draw [thick, dashed, red, -latex] plot [domain=-8:8] (\x, {3 + 0.5*\x - 3}); % red arrow
			\end{tikzpicture}
		\end{center}
	\columnbreak
		\begin{center}
			\begin{tikzpicture}[scale=0.11]
				\node at (0, 13) {\textbf{Ac.(-)}};
				% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
				\draw [thick, ->] (-10, 0) -- (10, 0) node [anchor=south] {$u_{t - 1}$}; % ut-1 axis
				\draw [thick, -] (-10, -10) -- (-10, 10) node [anchor=west] {$u_t$}; % ut axis
				\draw plot [only marks, mark=*, mark size=6, domain=-8:8, samples=25] (\x, {rnd*6 - 0.5*\x - 3}); % data points
				\draw [thick, dashed, red, -latex] plot [domain=-8:8] (\x, {3 - 0.5*\x - 3}); % red arrow
			\end{tikzpicture}
		\end{center}
	\end{multicols}
	\item \textbf{Test formales} - Durbin-Watson, Breusch-Godfrey, etc. Comúnmente, la hipótesis nula: $H_0$: No autocorrelación.
\end{itemize}

\subsection*{Corrección}

\begin{itemize}[leftmargin=*]
	\item Usar MCO con un estimador de la matriz de varianzas-covarianzas robusto a la heterocedasticidad y autocorrelación (HAC), por ejemplo, la propuesta de Newey-West.
	\item Usar Mínimos Cuadrados Generalizados. Suponiendo $y_t = \beta_0 + \beta_1 x_t + u_t$, con $u_t = \rho u_{t - 1} + \varepsilon_t$, donde $\lvert \rho \rvert < 1$ y $\varepsilon_t$ es ruido blanco.
	\begin{itemize}[leftmargin=*]
		\item Si $\rho$ es conocido, crear un modelo cuasi-diferenciado donde $u_t$ es ruido blanco y estimarlo por MCO.
		\item Si $\rho$ es desconocido, estimarlo -por ejemplo- por el método de Cochrane-Orcutt, crear un modelo cuasi-diferenciado donde $u_t$ es ruido blanco y estimarlo por MCO.
	\end{itemize}
\end{itemize}

\end{multicols}

\end{document}