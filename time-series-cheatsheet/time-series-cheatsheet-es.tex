\documentclass[10pt, a4paper, landscape]{extarticle}

% ----- packages -----
\usepackage{amsmath, amsfonts, amssymb} % better math
\usepackage{enumitem} % better lists
\usepackage{geometry} % margins
\usepackage{graphicx} % scaling
\usepackage{hyperref} % hyperlinks
\usepackage{multicol} % multiple columns
\usepackage{parskip} % paragraph spacing
\usepackage{scrlayer-scrpage} % page foot
\usepackage{tikz} % plots
\usepackage{titlesec} % titles
\usetikzlibrary{patterns} % patterns

% ----- random seed -----
\pgfmathsetseed{12}

% ----- custom commands -----
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\se}{\mathrm{ee}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}

% ----- page customization -----
\geometry{margin=1cm} % margins config
\pagenumbering{gobble} % remove page numeration
\setlength{\parskip}{0cm} % paragraph spacing
% title spacing
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1ex}{0ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}

% ----- document -----
\begin{document}

\cfoot{\href{https://github.com/marcelomijas/econometrics-cheatsheet}{\normalfont \footnotesize TS-24.5-ES - github.com/marcelomijas/econometrics-cheatsheet - Licencia CC-BY-4.0}}
\setlength{\footskip}{12pt}

\begin{multicols}{3}

\begin{center}
	\textbf{\LARGE \href{https://github.com/marcelomijas/econometrics-cheatsheet}{Cheat Sheet Series de Tiempo}}

	{\footnotesize Por Marcelo Moreno - Universidad Rey Juan Carlos}

	{\footnotesize The Econometrics Cheat Sheet Project}
\end{center}

\section*{Conceptos básicos}

\subsection*{Definiciones}

\textbf{Serie temporal} -  es una sucesión de observaciones cuantitativas de un fenómeno ordenadas en el tiempo.

Hay algunas variaciones de serie temporal:

\begin{itemize}[leftmargin=*]
	\item \textbf{Datos de panel} - consiste en una serie temporal para cada observación de una sección cruzada.
	\item \textbf{Secciones transversales agrupadas} - combina secciones cruzadas de diferentes periodos de tiempo.
\end{itemize}

		\textbf{Proceso estocástico} - es una secuencia de variables aleatorias que están indexadas en el tiempo.

\subsection*{Componentes de una serie temporal}

\begin{itemize}[leftmargin=*]
	\item \textbf{Tendencia} - es el movimiento general a l/p de una serie.
	\item \textbf{Variaciones estacionales} - son oscilaciones periódicas que son producidas en un período igual o inferior al año, y pueden ser fácilmente identificadas en diferentes años (usualmente son el resultado de la climatología).
	\item \textbf{Ciclo} - son oscilaciones periódicas que se producen en un periodo mayor al año (son resultado del ciclo económico).
	\item \textbf{Variaciones residuales} - son movimientos que no siguen una oscilación periódica identificable (resultado de fenómenos eventuales no permanentes que pueden afectar a la variable estudiada en un momento dado).
\end{itemize}

\subsection*{Tipos de modelos de series temporales}

\begin{itemize}[leftmargin=*]
	\item \textbf{Modelos estáticos} - la relación entre $y$ y $x$'s es contemporánea. Conceptualmente:
	\begin{center}
		$y_t = \beta_0 + \beta_1 x_t + u_t$
	\end{center}
	\item \textbf{Modelos de rezagos distribuidos} - la relación entre $y$ y $x$'s no es contemporánea. Conceptualmente:
	\begin{center}
		$y_t = \beta_0 + \beta_1 x_t + \beta_2 x_{t - 1} + \cdots + \beta_{s} x_{t - (s - 1)} + u_t$
	\end{center}
	El efecto acumulado a largo plazo en $y$ cuando $\Delta x$ es:
	\begin{center}
	 	$\beta_1 + \beta_2 + \cdots + \beta_{s}$
	\end{center}
 	\item \textbf{Modelos dinámicos} - un rezago de la variable dependiente es parte de las variables independientes (endogeneidad). Conceptualmente:
 	\begin{center}
 		$y_t = \beta_0 + \beta_1 y_{t - 1} + \cdots + \beta_s y_{t - s} + u_t$
 	\end{center}
	 \item Combinaciones de lo anterior, como modelos de rezagos distribuidos racionales (rezagos distribuidos + dinámicos).
\end{itemize}

\columnbreak

\section*{Supuestos y propiedades}

\subsection*{Supuestos MCO bajo series temporales}

Bajo estos supuestos, los estimadores de los parámetros MCO presentarán buenas propiedades. \textbf{Supuestos Gauss-Markov} extendidos en series temporales:

\begin{enumerate}[leftmargin=*, label=t\arabic*.]
	\item \textbf{Linealidad de parámetros y dependencia débil}.
	\begin{enumerate}[leftmargin=*, label=\alph*.]
		\item $y_t$ debe ser una función lineal de $\beta$'s.
		\item El proceso estocástico $\lbrace(x_t, y_t) : t = 1, 2, \ldots, T\rbrace$ es estacionario y débilmente dependiente.
	\end{enumerate}
	\item \textbf{No colinealidad perfecta}.
	\begin{itemize}[leftmargin=*]
		\item No hay variables independientes que sean constantes: $\Var(x_j) \neq 0, \; \forall j = 1, \ldots, k$
		\item No hay una relación lineal exacta entre variables independientes.
	\end{itemize}
	\item \textbf{Media condicional cero y correlación cero}.
	\begin{enumerate}[leftmargin=*, label=\alph*.]
		\item No hay errores sistemáticos: $\E(u \mid x_1, \ldots, x_k) = \E(u) = 0 \rightarrow$ \textbf{exogeneidad fuerte} (a implica b).
		\item No hay variables relevantes no incluidas en el modelo: $\Cov(x_j , u) = 0, \; \forall j = 1, \ldots, k \rightarrow$ \textbf{exogeneidad débil}.
	\end{enumerate}
	\item \textbf{Homocedasticidad}. La variabilidad de los resid. es igual para cualquier nivel de $x$: $\Var(u \mid x_1, \ldots, x_k) = \sigma^2_u$
	\item \textbf{No autocorrelación}. Los residuos no contienen información sobre otros residuos: \\ $\Corr(u_t, u_s \mid x_1, \ldots, x_k) = 0, \; \forall t \neq s$
	\item \textbf{Normalidad}. Los residuos son independientes e idénticamente distribuidos (\textbf{i.i.d.}): $u \sim \mathcal{N} (0, \sigma^2_u)$
			\item \textbf{Tamaño de datos}. El número de observaciones disponibles debe ser mayor a $(k + 1)$ parámetros a estimar. (Ya satisfecho bajo situaciones asintóticas)
\end{enumerate}

\subsection*{Propiedades asintóticas de MCO}

Bajo los supuestos del modelo econométrico y el Teorema Central del Límite:

\begin{itemize}[leftmargin=*]
	\item De t1 a t3a: MCO es \textbf{insesgado}. $\E(\hat{\beta}_j) = \beta_j$
	\item De t1 a t3: MCO es \textbf{consistente}. $\mathrm{plim}(\hat{\beta}_j) = \beta_j$ (a t3b sin t3a, exogeneidad débil, insesg. y consistente).
	\item De t1 a t5: \textbf{normalidad asintótica} de MCO (entonces, t6 es necesariamente satisfecho): $u \underset{a}{\sim} \mathcal{N} (0, \sigma^2_u)$
	\item De t1 a t5: \textbf{estimador insesgado} de $\sigma^2_u$. $\E(\hat{\sigma}^2_u) = \sigma^2_u$
	\item De t1 a t5: MCO es MELI (Mejor Estimador Lineal Insesgado, \textcolor{blue}{BLUE} en inglés) or \textbf{eficiente}. 
	\item De t1 a t6: contrastes de hipótesis e intervalos de confianza son fiables.
\end{itemize}

\columnbreak

\section*{Tendencia y estacionalidad}

\textbf{Regresión espuria} - es cuando la relación entre $y$ y $x$ es debida a factores que afectan a $y$ y que tienen correlación con $x$, $\Corr(x_j, u) \neq 0$. Es el \textbf{incumplimiento de t3}.

\subsection*{Tendencia}

Dos series temporales pueden tener la misma (o contraria) tendencia, lo que lleva a altos niveles de correlación. Esto provoca una falsa apariencia de causalidad, el problema es \textbf{regresión espuria}. Dado el modelo:

\begin{center}
	$y_t = \beta_0 + \beta_1 x_t + u_t$
\end{center}

donde:

\begin{center}
	$y_t = \alpha_0 + \alpha_1 \mathrm{Tendencia} + v_t$

	$x_t = \gamma_0 + \gamma_1 \mathrm{Tendencia} + v_t$
\end{center}

Añadir una tendencia al modelo puede resolver el problema:

\begin{center}
	$y_t = \beta_0 + \beta_1 x_t + \beta_2 \mathrm{Tendencia} + u_t$
\end{center}

Una tendencia puede ser lineal o no lineal (cuadrática, cúbica, exponencial, etc.)

Otra manera, es hacer uso del \textbf{filtro Hodrick-Prescott} y extraer la tendencia y el componente cíclico.

\subsection*{Estacionalidad}

\setlength{\multicolsep}{0pt}
\begin{multicols}{2}

	Una serie temporal puede manifestar estacionalidad. Esto es, que la serie está sujeta a variaciones estacionales o patrones usualmente relacionados al clima.

	Por ejemplo, el PIB (negro) es usualmente mayor en verano y menor en invierno. Serie ajustada estacionalmente ({\color{red} rojo}) en comparación.

\columnbreak

	\begin{tikzpicture}[scale=0.18]
		% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
		\draw [thick, <->] (-10, 10) node [anchor=south] {$y$} -- (-10, -10) -- (10, -10) node [anchor=north] {$t$}; %axis
		\draw [thick, black]
		(-10.0, -7.206) -- 
		(-9.5, -5.190) -- 
		(-9.0, -7.500) -- 
		(-8.5, -2.381) -- 
		(-8.0, -3.969) -- 
		(-7.5, -1.160) -- 
		(-7.0, -4.580) -- 
		(-6.5, 0.855) -- 
		(-6.0, -1.526) -- 
		(-5.5, -0.305) -- 
		(-5.0, -4.519) -- 
		(-4.5, -0.488) -- 
		(-4.0, -2.320) -- 
		(-3.5, -0.427) -- 
		(-3.0, -4.213) -- 
		(-2.5, 0.366) -- 
		(-2.0, -1.709) -- 
		(-1.5, -0.549) -- 
		(-1.0, -4.396) -- 
		(-0.5, 1.099) -- 
		(0.0, -1.038) -- 
		(0.5, 1.282) -- 
		(1.0, -2.870) -- 
		(1.5, 1.709) -- 
		(2.0, -1.038) -- 
		(2.5, 1.526) -- 
		(3.0, -1.832) -- 
		(3.5, 3.358) -- 
		(4.0, 1.099) -- 
		(4.5, 4.213) -- 
		(5.0, 0.916) -- 
		(5.5, 6.290) -- 
		(6.0, 4.396) -- 
		(6.5, 6.595) -- 
		(7.0, 3.419) -- 
		(7.5, 8.000) -- 
		(8.0, 6.106) -- 
		(8.5, 6.900);
		\draw [thick, red] 
		(-10.0, -6.2061) -- 
		(-9.5, -6.0018) -- 
		(-9.0, -6.1000) -- 
		(-8.5, -5.0817) -- 
		(-8.0, -3.9095) -- 
		(-7.5, -3.0603) -- 
		(-7.0, -3.0002) -- 
		(-6.5, -2.4550) -- 
		(-6.0, -2.5267) -- 
		(-5.5, -2.3053) -- 
		(-5.0, -2.5191) -- 
		(-4.5, -2.4885) -- 
		(-4.0, -2.3206) -- 
		(-3.5,  -2.4275) -- 
		(-3.0, -2.2137) -- 
		(-2.5, -1.6664) -- 
		(-2.0, -2.0099) -- 
		(-1.5, -1.8496) -- 
		(-1.0, -1.3969) -- 
		(-0.5, -1.0992) -- 
		(0.0, -1.0382) -- 
		(0.5, -1.2824) -- 
		(1.0, -1.0002) -- 
		(1.5, -0.8099) -- 
		(2.0, -0.6382) -- 
		(2.5, -0.6267) -- 
		(3.0, 0.6321) -- 
		(3.5, 1.0588) -- 
		(4.0, 1.3992) -- 
		(4.5, 2.2137) -- 
		(5.0, 2.5160) -- 
		(5.5, 3.5901) -- 
		(6.0, 4.0969) -- 
		(6.5, 5.2954) -- 
		(7.0, 5.3198) -- 
		(7.5, 6.2000) -- 
		(8.0, 6.9069) -- 
		(8.5, 7.2008);
	\end{tikzpicture}

\end{multicols}

\begin{itemize}[leftmargin=*]
	\item Este problema es \textbf{regresión espuria}. Un ajuste estacional puede solucionarlo.
\end{itemize}

Un \textbf{ajuste estacional} sencillo es crear variables estacionales binarias y añadirlas al modelo. Por ejemplo, una serie trimestral ($Qq_t$ son variables binarias):

\begin{center}
	$y_t = \beta_0 + \beta_1 Q2_t + \beta_2 Q3_t + \beta_3 Q4_t + \beta_4 x_{1t} + \cdots + \beta_k x_{kt} + u_t$
\end{center}

Otro método es ajustar estacionalmente (sa) las variables, y entonces, hacer la regresión con las variables ajustadas:

\begin{center}
	$z_t = \beta_0 + \beta_1 Q2_t + \beta_2 Q3_t + \beta_3 Q4_t  + v_t \rightarrow \hat{v}_t + \E(z_t) = \hat{z}_t^{sa}$

	$\hat{y}_t^{sa} = \beta_0 + \beta_1 \hat{x}_{1t}^{sa} + \cdots + \beta_k \hat{x}_{kt}^{sa} + u_t$
\end{center}

Hay métodos mucho mejores y complejos para ajustar estacionalmente, como el \textbf{X-13ARIMA-SEATS}.

\columnbreak

\section*{Autocorrelación}

El residuo de cualquier observación, $u_t$, está correlacionado con el residuo de cualquier otra observación. Las observaciones no son independientes. Es el \textbf{incumplimiento} de \textbf{t5}.

\begin{center}
	$\Corr(u_t, u_s \mid x_1, \ldots, x_k) = \Corr(u_t, u_s) \neq 0, \; \forall t \neq s$
\end{center}

\subsection*{Consecuencias}

\begin{itemize}[leftmargin=*]
	\item Estimadores MCO son insesgados.
	\item Estimadores MCO son consistentes.
	\item MCO ya \textbf{no es eficiente}, pero sigue siendo ELI (Estimador Lineal Insesgado).
	\item La \textbf{estimación de la varianza} de los estimadores es \textbf{sesgada}: la construcción de intervalos de confianza y contraste de hipótesis no son fiables.
\end{itemize}

\subsection*{Detección}

\begin{itemize}[leftmargin=*]
	\item \textbf{Gráficos de dispersión} - buscar patrones de dispersión en $u_{t - 1}$ vs. $u_t$.
	\setlength{\multicolsep}{0pt}
	\setlength{\columnsep}{6pt}
	\begin{multicols}{3}
		\begin{center}
			\begin{tikzpicture}[scale=0.11]
				\node at (0, 13) {\textbf{Ac.}};
				% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
				\draw [thick, ->] (-10, 0) -- (10, 0) node [anchor=south] {$u_{t - 1}$}; % ut-1 axis
				\draw [thick, -] (-10, -10) -- (-10, 10) node [anchor=west] {$u_t$}; % ut axis
				\draw plot [only marks, mark=*, mark size=6, domain=-8:8, samples=50] (\x, {rnd*6 + (-2*(\x)^2 + 40)*0.1}); % data points
				\draw [thick, dashed, red, -latex] plot [domain=-8:8] (\x, {3 + (-2*(\x)^2 + 40)*0.1}); % red arrow
			\end{tikzpicture}
		\end{center}

	\columnbreak

		\begin{center}
			\begin{tikzpicture}[scale=0.11]
				\node at (0, 13) {\textbf{Ac.(+)}};
				% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
				\draw [thick, ->] (-10, 0) -- (10, 0) node [anchor=north] {$ u_{t - 1}$}; % ut-1 axis
				\draw [thick, -] (-10, -10) -- (-10, 10) node [anchor=west] {$u_t$}; % ut axis
				\draw plot [only marks, mark=*, mark size=6, domain=-8:8, samples=25] (\x, {rnd*6 + 0.5*\x - 3}); % data points
				\draw [thick, dashed, red, -latex] plot [domain=-8:8] (\x, {3 + 0.5*\x - 3}); % red arrow
			\end{tikzpicture}
		\end{center}

	\columnbreak

		\begin{center}
			\begin{tikzpicture}[scale=0.11]
				\node at (0, 13) {\textbf{Ac.(-)}};
				% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
				\draw [thick, ->] (-10, 0) -- (10, 0) node [anchor=south] {$u_{t - 1}$}; % ut-1 axis
				\draw [thick, -] (-10, -10) -- (-10, 10) node [anchor=west] {$u_t$}; % ut axis
				\draw plot [only marks, mark=*, mark size=6, domain=-8:8, samples=25] (\x, {rnd*6 - 0.5*\x - 3}); % data points
				\draw [thick, dashed, red, -latex] plot [domain=-8:8] (\x, {3 - 0.5*\x - 3}); % red arrow
			\end{tikzpicture}
		\end{center}

	\end{multicols}

	\begin{multicols}{2}

		\item \textbf{Correlograma} - compuesto de la función de autocorrelación (FAC) y el FAC parcial (FACP).

	\columnbreak

		\begin{itemize}[leftmargin=*]
			\item Eje Y: correlación [-1, 1].
			\item Eje X: número de retardo.
			\item Líneas azules: $\pm 1.96/T^{0.5}$
		\end{itemize}
	\end{multicols}

	\begin{center}
		\begin{tikzpicture}[scale=0.22]
			% acf plot
			\node at (17.5, 23) {\tiny \textbf{ACF}};
			\node at (-1.5, 22) {\tiny 1};
			\node at (-1.5, 17) {\tiny 0};
			\node at (-1.5, 12) {\tiny -1};
			\draw [step=2, gray, thin] (0, 12) rectangle (35, 22);
			\draw [thick, |->] (0, 22) -- (0, 12) -- (35, 12);
			\fill [red] (1.5, 17) rectangle (2, 21.9);
			\fill [red] (3.5, 17) rectangle (4, 21.7);
			\fill [red] (5.5, 17) rectangle (6, 21.5);
			\fill [red] (7.5, 17) rectangle (8, 21.2);
			\fill [red] (9.5, 17) rectangle (10, 20.8);
			\fill [red] (11.5, 17) rectangle (12, 20.6);
			\fill [red] (13.5, 17) rectangle (14, 20.4);
			\fill [red] (15.5, 17) rectangle (16, 20.3);
			\fill [red] (17.5, 17) rectangle (18, 20.2);
			\fill [red] (19.5, 17) rectangle (20, 20.1);
			\fill [red] (21.5, 17) rectangle (22, 20.0);
			\fill [red] (23.5, 17) rectangle (24, 19.9);
			\fill [red] (25.5, 17) rectangle (26, 19.8);
			\fill [red] (27.5, 17) rectangle (28, 19.7);
			\fill [red] (29.5, 17) rectangle (30, 19.6);
			\fill [red] (31.5, 17) rectangle (32, 19.5);
			\fill [red] (33.5, 17) rectangle (34, 19.3);
			\draw [blue, thin] (0, 18.5) -- (35, 18.5);
			\draw [dashed, thin] (0, 17) -- (35, 17);
			\draw [blue, thin] (0, 15.5) -- (35, 15.5);
			% pacf plot
			\node at (17.5, 11) {\tiny \textbf{PACF}};
			\node at (-1.5, 10) {\tiny 1};
			\node at (-1.5, 5) {\tiny 0};
			\node at (-1.5, 0) {\tiny -1};
			\draw [step=2, gray, thin] (0, 0) rectangle (35, 10);
			\draw [thick, |->] (0, 10) -- (0, 0) -- (35, 0);
			\fill [red] (1.5, 5) rectangle (2, 9.9);
			\fill [red] (3.5, 5) rectangle (4, 3.0);
			\fill [red] (5.5, 5) rectangle (6, 4.5);
			\fill [red] (7.5, 5) rectangle (8, 4.2);
			\fill [red] (9.5, 5) rectangle (10, 5.8);
			\fill [red] (11.5, 5) rectangle (12, 4.7);
			\fill [red] (13.5, 5) rectangle (14, 4.5);
			\fill [red] (15.5, 5) rectangle (16, 4.8);
			\fill [red] (17.5, 5) rectangle (18, 5.6);
			\fill [red] (19.5, 5) rectangle (20, 4.9);
			\fill [red] (21.5, 5) rectangle (22, 4.7);
			\fill [red] (23.5, 5) rectangle (24, 4.6);
			\fill [red] (25.5, 5) rectangle (26, 5.4);
			\fill [red] (27.5, 5) rectangle (28, 4.9);
			\fill [red] (29.5, 5) rectangle (30, 4.8);
			\fill [red] (31.5, 5) rectangle (32, 4.8);
			\fill [red] (33.5, 5) rectangle (34, 5.1);
			\draw [blue, thin] (0, 6.5) -- (35, 6.5);
			\draw [dashed, thin] (0, 5) -- (35, 5);
			\draw [blue, thin] (0, 3.5) -- (35, 3.5);
		\end{tikzpicture}
	\end{center}

	Conclusiones difieren entre procesos de autocorrelación.

\columnbreak

	\begin{itemize}[leftmargin=*]
		\item \textbf{Proceso MA($q$)}. \textbf{FAC}: sólo los $q$ primeros coeficientes son significativos, el resto se anulan bruscamente. \textbf{FACP}: decrecimiento rápido exponencial atenuado u ondas sinusoidales.
		\item \textbf{Proceso AR($p$)}. \textbf{FAC}: decrecimiento rápido exponencial atenuado u ondas sinusoidales. \textbf{FACP}: sólo los $p$ primeros coeficientes son significativos, el resto se anulan bruscamente.
		\item \textbf{Proceso ARMA($p, q$)}. \textbf{FAC} y \textbf{FACP}: los coeficientes no se anulan bruscamente y presentan un decrecimiento rápido.
	\end{itemize}

	Si los coeficientes de la FAC no decaen rápidamente, hay claro indicio de falta de estacionariedad en media, lo que llevaría a tomar primeras diferencias en la serie original.

	\item \textbf{Formal tests} - Generalmente, $H_0$: No autocorrelación.

	Suponiendo que $u_t$ sigue un proceso AR(1):
	
	\begin{center}
		$u_t = \rho_1 u_{t - 1} + \varepsilon_t$
	\end{center}

	donde $\varepsilon_t$ es ruido blanco.
	
	\begin{itemize}[leftmargin=*]
		\item \textbf{Prueba t AR(1)} (regresores exógenos):
		\begin{center}
			$t = \frac{\hat{\rho}_1}{\se(\hat{\rho}_1)} \sim t_{T - k - 1, \alpha/2}$
		\end{center}
		\begin{itemize}[leftmargin=*]
			\item $H_1$: Autocorrelación de orden uno, AR(1).
		\end{itemize}
		\item \textbf{Estadístico Durbin-Watson} (regresores exógenos y normalidad de residuos):
		\begin{center}
			$d = \frac{\sum_{t=2}^n (\hat{u}_t - \hat{u}_{t - 1})^2}{\sum_{t=1}^n \hat{u}_t^2} \approx 2 \cdot (1 - \hat{\rho}_1), \; 0 \leq d \leq 4$
		\end{center}
		\begin{itemize}[leftmargin=*]
			\item $H_1$: Autocorrelación de orden uno, AR(1).
		\end{itemize}
		\begin{center}
			\scalebox{0.8}{
				\begin{tabular}{| c | c | c | c |}
					\hline
					$d =$          & 0 & 2 & 4  \\ \hline
					$\rho \approx$ & 1 & 0 & -1 \\ \hline
				\end{tabular}
			}
			\begin{tikzpicture}[scale=0.28]
				\fill [pattern=north east lines, pattern color=gray] (5, 0) rectangle  (9, 9);
				\draw (5, 0) -- (5, 9);
				\draw (9, 0) -- (9, 9);
				\fill [pattern=north east lines, pattern color=gray] (16, 0) rectangle (20, 9);
				\draw (16, 0) -- (16, 9);
				\draw (20, 0) -- (20, 9);
				\draw [thick] (0, 9) -- (0, 0) -- (25, 0);
				\draw [dashed] (12.5, 0) -- (12.5, 9);
				\node at (-1.5, 8.5) {\scalebox{1.2}{\tiny $f(d)$}};
				\node at (0, -0.6) {\scalebox{1.2}{\tiny 0}};
				\node at (5, -0.6) {\scalebox{1.2}{\tiny $d_L$}};
				\node at (9, -0.6) {\scalebox{1.2}{\tiny $d_U$}};
				\node at (12.5, -0.6) {\scalebox{1.2}{\tiny 2}};
				\node at (16.7, -1) {\scalebox{1.1}{\tiny \rotatebox{-20}{$4 - d_U$}}};
				\node at (20.7, -1) {\scalebox{1.1}{\tiny \rotatebox{-20}{$4 - d_L$}}};
				\node at (25, -0.6) {\scalebox{1.2}{\tiny 4}};
				\node at (2.5, 5.5) {\scalebox{1.2}{\tiny Rech. $H_0$}};
				\node at (2.5, 4.5) {\scalebox{1.2}{\tiny AR(+)}};
				\node [text=red] at (7,4.5) {\scalebox{1.1}{\tiny \rotatebox{-70}{\textbf{INCONCLUYENTE}}}};
				\node at (12.5, 5.5) {\scalebox{1.2}{\tiny Acceptar $H_0$}};
				\node at (12.5, 4.5) {\scalebox{1.2}{\tiny No AR}};
				\node [text=red] at (18,4.5) {\scalebox{1.1}{\tiny \rotatebox{-70}{\textbf{INCONCLUYENTE}}}};
				\node at (22.5, 5.5) {\scalebox{1.2}{\tiny Rech. $H_0$}};
				\node at (22.5, 4.5) {\scalebox{1.2}{\tiny AR(-)}};
			\end{tikzpicture}
		\end{center}
		\item \textbf{h de Durbin} (regresores endógenos):
		\begin{center}
			$h = \hat{\rho} \cdot \sqrt{\frac{T}{1 - T \cdot \upsilon}}$
		\end{center}
		donde $\upsilon$ es la varianza estimada del coeficiente asociado a la variable endógena.
		\begin{itemize}[leftmargin=*]
			\item $H_1$: Autocorrelación de orden uno, AR(1).
		\end{itemize}
		\item \textbf{Prueba Breusch-Godfrey} (regresores endógenos): puede detectar procesos MA($q$) y AR($p$) ($\varepsilon_t$ ruido b.):
		\begin{itemize}[leftmargin=*]
			\item MA($q$): $u_t = \varepsilon_t - m_1 u_{t - 1} - \cdots - m_q u_{t - q}$
			\item AR($p$): $u_t = \rho_1 u_{t - 1} + \cdots + \rho_p u_{t - p} + \varepsilon_t$
		\end{itemize}
		Bajo $H_0$: No autocorrelación:
		\begin{center}
			$\hfill T \cdot R^2_{\hat{u}_t} \underset{a}{\sim} \chi^2_q \hfill \textbf{or} \hfill T \cdot R^2_{\hat{u}_t} \underset{a}{\sim} \chi^2_p \hfill$
		\end{center}
		\begin{itemize}[leftmargin=*]
			\item $H_1$: Autocorrelación de orden $q$ (ó $p$).
		\end{itemize}
		\item \textbf{Prueba Ljung-Box Q}:
		\begin{itemize}[leftmargin=*]
			\item $H_1$: Existe autocorrelación.
		\end{itemize}
	\end{itemize}

\end{itemize}

\subsection*{Corrección}

\begin{itemize}[leftmargin=*]
	\item Usar MCO con un estimador de la matriz de varianzas-covarianzas \textbf{robusto a la heterocedasticidad y autocorrelación} (HAC), por ejemplo, la propuesta de \textbf{Newey-West}.
	\item Usar \textbf{Mínimos Cuadrados Generalizados} (MCG). Suponiendo $y_t = \beta_0 + \beta_1 x_t + u_t$, con $u_t = \rho u_{t - 1} + \varepsilon_t$, donde $\lvert \rho \rvert < 1$ y $\varepsilon_t$ es \underline{ruido blanco}.
	\begin{itemize}[leftmargin=*]
		\item Si $\rho$ es \textbf{conocido}, usar \textbf{modelo cuasi-diferenciado}:
		\begin{center}
			$y_t - \rho y_{t - 1} = \beta_0 (1 - \rho) + \beta_1 (x_t - \rho x_{t - 1}) + u_t - \rho u_{t - 1}$
			
			$y_t^* = \beta_0^* + \beta_1' x_t^* + \varepsilon_t$
		\end{center}
		donde $\beta_1' = \beta_1$; y estimarlo por MCO.
		\item Si $\rho$ es \textbf{desconocido}, estimarlo -por ejemplo- el \textbf{método iterativo de Cochrane-Orcutt} (el método de Prais-Winsten también es bueno):
		\begin{enumerate}[leftmargin=*]
			\item Obtener $\hat{u}_t$ del modelo original.
			\item Estimar $\hat{u}_t = \rho \hat{u}_{t-1} + \varepsilon_t$ y obtener $\hat{\rho}$.
			\item Crear un modelo cuasi-diferenciado:
			\begin{center}
				$y_t - \hat{\rho} y_{t - 1} = \beta_0 (1 - \hat{\rho}) + \beta_1 (x_t - \hat{\rho} x_{t - 1}) + u_t - \hat{\rho} u_{t - 1}$
				
				$y_t^* = \beta_0^* + \beta_1' x_t^* + \varepsilon_t$
			\end{center}
			donde $\beta_1' = \beta_1$; y estimarlo por MCO.
			\item Obtener $\hat{u}_t^* = y_t - (\hat{\beta}_0^* + \hat{\beta}_1' x_t) \neq y_t - (\hat{\beta}_0^* + \hat{\beta}_1' x_t^*)$.
			\item Repetir desde el paso 2. El algoritmo termina cuando los parámetros estimados varían muy poco entre iteraciones.
		\end{enumerate}
	\end{itemize}
	\item Si no se arregla, buscar \textbf{fuerte dependencia} en la serie.
\end{itemize}

\section*{Estacionariedad y dependencia débil}

Estacionariedad es estabilidad de las distribuciones conjuntas de probabilidad de un proceso a medida que este progresa el tiempo. Permite identificar correctamente las relaciones -inalteradas en el tiempo- entre variables.

\subsection*{Procesos estacionarios y no estacionarios}

\begin{itemize}[leftmargin=*]
	\item \textbf{Proceso estacionario} (estacionariedad fuerte) - la dist. de prob. es estable en el tiempo: si se toma cualquier colección de variables aleatorias, y se mueven $h$ periodos, la distribución conjunta de probabilidad debe permanecer inalterada. Facilita el análisis y modelado.

\columnbreak

	\item \textbf{Proceso no estacionario} - por ejemplo, una serie con tendencia, donde al menos la media cambia con el tiempo.
	\item \textbf{Proceso estacionario en covarianza} - es una forma más débil de estacionariedad:
	\begin{itemize}[leftmargin=*]
		\item $\E(x_t)$ es constante.
		\item $\Var(x_t)$ es constante.
		\item Para cualquier $t$,  $h \geq 1$, la $\Cov(x_t, x_{t + h})$ depende sólo de $h$, no de $t$.
	\end{itemize}
\end{itemize}

\subsection*{Series temporales de dependencia débil}

Es importante porque reemplaza el requisito de muestreo aleatorio, dando por supuesto la validez del Teorema Central del Límite (requiere estacionariedad y una forma de dependencia débil). Los procesos débilmente dependientes también se conocen como, \textbf{integrado de orden cero}, I(0).

\begin{itemize}[leftmargin=*]
	\item \textbf{Dependencia débil} - restringe cuán cercana la relación entre $x_t$ y $x_{t+h}$ puede ser a medida que la distancia temporal entre las series aumenta ($h$).
\end{itemize}

Un \textbf{proceso estacionario} $\lbrace x_t : t = 1, 2, \ldots, T \rbrace$ es débilmente dependiente cuando $x_t$ y $x_{t + h}$ son casi independientes a medida que $h$ aumenta sin límite.

Un \textbf{proceso estacionario en covarianza} es débilmente dependiente si la correlación entre $x_t$ y $x_{t + h}$ tiende a $0$ lo suficientemente rápido cuando $h \rightarrow \infty$ (no están asintóticamente correlacionados).

Algunos ejemplos de series estacionarias y débilmente dependientes:

\begin{itemize}[leftmargin=*]
	\item \textbf{Media móvil} - $\lbrace x_t \rbrace$ es una media móvil de orden uno MA($q$):
	\begin{center}
		$x_t = e_t + m_1 e_{t - 1} + \cdots + m_q e_{t - q}$
	\end{center}
	donde $\lbrace e_t : t = 0, 1, \ldots, T \rbrace$ es una secuencia \textsl{i.i.d.} con media cero y varianza $\sigma^2_e$.
	\item \textbf{Proceso autorregresivo} - $\lbrace x_t \rbrace$ es un proceso autorregresivo de orden uno AR($p$):
	\begin{center}
		$x_t = \rho_1 x_{t - 1} + \cdots + \rho_p x_{t - p} + e_t$
	\end{center}
	donde $\lbrace e_t: t = 1, 2, \ldots, T \rbrace$ es una secuencia \textsl{i.i.d.} con media cero y varianza $\sigma^2_e$.

	Si $\lvert \rho_1 \rvert < 1$, entonces $\lbrace x_t \rbrace$ es un proceso AR(1) que es débilmente dependiente. Es estacionario en covarianza, $\Corr(x_t, x_{t - 1}) = \rho_1$.
	\item \textbf{Proceso ARMA} - es una combinación de los dos anteriores. $\lbrace x_t \rbrace$ es un ARMA($p, q$):
	\begin{center}
		$x_t = e_t + m_1 e_{t - 1} + \cdots + m_q e_{t - q} + \rho_1 x_{t - 1} + \cdots + \rho_p x_{t - p}$
	\end{center}
\end{itemize}

Una serie con tendencia no puede ser estacionaria, pero puede ser débilmente dependiente (y estacionaria si la serie es filtrada de tendencia).

\columnbreak

\section*{Series temporales de depend. fuerte}

La mayoría del tiempo, las series económicas presentan dependencia fuerte (o fuerte persistencia temporal). Algunos casos especiales de procesos de \textbf{raíz unitaria}, I(1):

\begin{itemize}[leftmargin=*]
		\item \textbf{Paseo aleatorio} - un proceso AR(1) con $\rho_1 = 1$.
	\begin{center}
		$y_t = y_{t - 1} + e_t$
	\end{center}
	donde $\lbrace e_t : t = 1, 2, \ldots, T \rbrace$ es una secuencia \textsl{i.i.d.} con media cero y varianza $\sigma^2_e$.
	
	\item \textbf{Paseo aleatorio con deriva} - un proceso AR(1) con $\rho_1 = 1$ y una constante.
	\begin{center}
		$y_t = \beta_0 + y_{t - 1} + e_t$
	\end{center}
	donde $\lbrace e_t : t = 1, 2, \ldots, T \rbrace$ es una secuencia \textsl{i.i.d.} con media cero y varianza $\sigma^2_e$.
\end{itemize}

\subsection*{Detección de I(1)}

\begin{itemize}[leftmargin=*]
	\item \textbf{Prueba Aumentada de Dickey-Fuller} (ADF) - donde $H_0$: el proceso es raíz unitaria, I(1).
	\item \textbf{Prueba Kwiatkowski–Phillips–Schmidt–Shin} (KPSS) - donde $H_0$: el proceso no es raíz unitaria, I(0).
	\item \textbf{Prueba Phillips-Perron} (PP) - donde $H_0$: el proceso no es raíz unitaria, I(0).
\end{itemize}

\subsection*{Transformar raíz unitaria a depend. débil}

Los procesos de \textbf{raíz unitaria} son \textbf{integrados de orden uno}, I(1). Esto significa que \textbf{la primera diferencia} del proceso es \textbf{débilmente dependiente} ó I(0) (y usualmente, estacionaria). Por ejemplo, un paseo aleatorio:

\begin{multicols}{2}

\begin{center}
	$\Delta y_t = y_t - y_{t - 1} = e_t$
\end{center}

donde $\lbrace e_t \rbrace = \lbrace \Delta y_t \rbrace$  es \textsl{i.i.d.} \\

Tomar la primera diferencia de una serie también elimina su tendencia. \\

Por ejemplo, una serie con tendencia (negro), y su primera diferencia ({\color{red} rojo}).

\columnbreak

\begin{tikzpicture}[scale=0.18]
	% \draw [step=2, gray, very thin] (-10, -10) grid (10, 10); % grid
	\draw [thick, <->] (-10, 10) node [anchor=south west] {$y, {\color{red} \Delta y}$} -- (-10, -10) -- (10, -10) node [anchor=north] {$t$}; %axis
	\draw [thick, black]
	(-10.0, -8.000) -- 
	(-9.5, -7.541) -- 
	(-9.0, -7.284) -- 
	(-8.5, -6.795) -- 
	(-8.0, -6.429) -- 
	(-7.5, -6.048) -- 
	(-7.0, -5.953) -- 
	(-6.5, -5.486) -- 
	(-6.0, -5.281) -- 
	(-5.5, -4.840) -- 
	(-5.0, -4.326) -- 
	(-4.5, -4.013) -- 
	(-4.0, -3.758) -- 
	(-3.5, -3.529) -- 
	(-3.0, -3.056) -- 
	(-2.5, -2.896) -- 
	(-2.0, -2.416) -- 
	(-1.5, -1.913) -- 
	(-1.0, -1.888) -- 
	(-0.5, -1.166) -- 
	(0.0, -0.530) -- 
	(0.5, -0.282) -- 
	(1.0, 0.032) -- 
	(1.5, 0.491) -- 
	(2.0, 0.748) -- 
	(2.5, 0.805) -- 
	(3.0, 1.016) -- 
	(3.5, 1.439) -- 
	(4.0, 1.810) -- 
	(4.5, 2.247) -- 
	(5.0, 2.668) -- 
	(5.5, 3.052) -- 
	(6.0, 3.586) -- 
	(6.5, 4.322) -- 
	(7.0, 4.913) -- 
	(7.5, 5.704) -- 
	(8.0, 6.081) -- 
	(8.5, 6.431);
	\draw [thick, red]
	(-9.5, 1.283) -- 
	(-9.0, -2.799) -- 
	(-8.5, 1.889) -- 
	(-8.0, -0.595) -- 
	(-7.5, -0.299) -- 
	(-7.0, -6.074) -- 
	(-6.5, 1.454) -- 
	(-6.0, -3.864) -- 
	(-5.5, 0.926) -- 
	(-5.0, 2.393) -- 
	(-4.5, -1.655) -- 
	(-4.0, -2.843) -- 
	(-3.5, -3.373) -- 
	(-3.0, 1.572) -- 
	(-2.5, -4.765) -- 
	(-2.0, 1.703) -- 
	(-1.5, 2.186) -- 
	(-1.0, -7.487) -- 
	(-0.5, 6.607) -- 
	(0.0, 4.869) -- 
	(0.5, -2.985) -- 
	(1.0, -1.632) -- 
	(1.5, 1.283) -- 
	(2.0, -2.804) -- 
	(2.5, -6.847) -- 
	(3.0, -3.723) -- 
	(3.5, 0.547) -- 
	(4.0, -0.483) -- 
	(4.5, 0.834) -- 
	(5.0, 0.526) -- 
	(5.5, -0.246) -- 
	(6.0, 2.816) -- 
	(6.5, 6.875) -- 
	(7.0, 3.961) -- 
	(7.5, 8.000) -- 
	(8.0, -0.356) -- 
	(8.5, -0.925);
\end{tikzpicture}

\end{multicols}

Cuando una serie I(1) es estrictamente positiva, se suele transformar a logaritmos antes de tomar primeras diferencias. Esto es, para obtener el cambio porcentual (aprox.) de la serie:

\begin{center}
	$\Delta \log(y_t) = \log(y_t) - \log(y_{t - 1}) \approx \dfrac{y_t - y_{t - 1}}{y_{t - 1}}$
\end{center}

\columnbreak

\section*{Cointegración}

Cuando \textbf{dos series son I(1), pero una combinación lineal de estas es I(0)}. Si es el caso, la regresión de una serie sobre la otra no es espuria, sino que expresa algo sobre la relación a largo plazo. Se llama cointegradas a las variables que tienen una tendencia estocástica común.

Por ejemplo: $\lbrace x_t \rbrace$ y $\lbrace y_t \rbrace$ son I(1), pero $y_t - \beta x_t = u_t$ donde $\lbrace u_t \rbrace$ es I(0). ($\beta$ toma el nombre de parámetro cointegrador).

\section*{Heterocedasticidad en series temp.}

Afecta al \textbf{supuesto t4}, lo que lleva a que \textbf{MCO no sea eficiente}.

Algunas pruebas que funcionan pueden ser la de Breusch-Pagan o la de White, donde $H_0$: No heterocedasticidad. Es \textbf{importante} que \textbf{no} haya \textbf{autocorrelación} para el correcto funcionamiento de las pruebas (así que, primero es imperativo probar la existencia de autocorrelación).

\subsection*{ARCH}

Heterocedasticidad condicional autorregresiva (ARCH), es un modelo para analizar una forma de heteroced. dinámica, donde la varianza del error sigue un proceso AR($p$).

Dado el modelo: $y_t = \beta_0 + \beta_1 z_t + u_t$	donde, hay AR(1) y heterocedasticidad:

\begin{center}
	$\E(u^2_t \mid u_{t - 1}) = \alpha_0 + \alpha_1 u^2_{t - 1}$
\end{center}

\subsection*{GARCH}

La heterocedasticidad condicional autorregresiva general (GARCH), es un modelo similar a ARCH, pero en este caso, la varianza del error sigue un proceso ARMA($p, q$).

\section*{Suavizado exponencial}

\begin{center}
	$f_t = \alpha y_t + (1 - \alpha) f_{t - 1}$
\end{center}

donde $0 < \alpha < 1$ es el parámetro de suavizado.

\section*{Predicciones}

Dos tipos de predicciones:

\begin{itemize}[leftmargin=*]
	\item Valor medio de $y$ para un valor específico de $x$.
	\item Valor individual de $y$ para un valor específico de $x$.
\end{itemize}

Si los valores de las variables ($x$) se aproximan al valor medio ($\overline{x}$), la amplitud del intervalo de confianza de la predicción será menor.

\end{multicols}

\end{document}